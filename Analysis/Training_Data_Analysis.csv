Paper,Author,"1/ Are the languages the models have seen during pre-training discussed in the paper? If yes, where and in what context?",2/ Is the impact of models pre-training discussed with relation to in-context learning ability?,"SCORE


Minimal Mention (1): The paper barely discusses the pre-training data or its impact on in-context learning.

Limited Mention (2): The paper mentions pre-training data but lacks depth or specific analysis related to in-context learning. 

Moderate Discussion (3): The paper provides a reasonable discussion on pre-training data and its impact on in-context learning, with some specific examples.

Detailed Discussion (4): The paper offers a detailed discussion on pre-training data, including specific languages/scripts and their impact on in-context learning.

Comprehensive Analysis (5): The paper thoroughly analyzes pre-training data, providing extensive details on languages/scripts and a robust examination of their impact on in-context learning.","Critical Analysis:
Whether the study effectively discuss the model training data with respect to the results of the experiment.",Language Coverage,Language Notes,Models
Language Models are Few-shot Multilingual Learners,"Winata et al., 2021","Explicitly mentions languages in training/experiments: models trained mostly on English, tested for multilingual capability in English, French, German, and Spanish.
Focus on how models (GPT, T5) handle languages in monolingual and cross-lingual settings. Notes that models adapt better to structurally similar languages to English, suggesting some exposure during training.","Pre-training's impact is key. It looks at how language models pre-trained mostly on English handle multilingual few-shot tasks without extra training. Larger models do better, hinting that more and varied pre-training data helps them generalize across languages. This shows up in cross-lingual tasks, with models tested on translating or understanding language pairs they weren't fine-tuned on.",5,"The paper shows a reasonably detailed discussion about the effectiveness of using pre-trained models on multilingual NLU tasks. However, while it highlights the capabilities of these models to perform with few-shot examples in languages that are not part of the extensive training regime, it doesn’t deeply analyze or disclose the specific makeup or comprehensiveness of the non-English data in the training sets.

The effectiveness in non-English languages, as presented, seems to hinge more on the models’ architectural capabilities and size rather than a robust, linguistically diverse training dataset. This might pose limitations in evaluating the true linguistic adaptability of the models beyond surface-level patterns or similarities to English.","English, French, German, Spanish",closely related to English,"GPT-2, GPT-NEO, T5, BART, XLM-R"
Few-shot Learning with Multilingual Language Models,"Lin et al., 2022","The paper specifically mentions that the pre-training data is up-sampled for medium and low-resource languages to create a balanced language distribution, which is crucial for developing multilingual capabilities in generative models.","Pre-training's impact is a major focus. Models, especially the largest one with 7.5 billion parameters, show advanced few- and zero-shot learning across multiple languages, outperforming benchmarks like GPT-3 in tasks like multilingual commonsense reasoning and natural language inference. This boost is attributed to diverse and extensive pre-training, providing a solid foundation for cross-lingual transfer and adaptable in-context learning.",5,"The paper provides a detailed analysis of the effects of training data diversity on the experimental results. It is highlighted that the enriched multilingual pre-training enables the models to perform well in a variety of non-English languages, even in zero-shot settings. However, while the paper emphasizes the breadth of languages covered and the balance achieved through up-sampling, there is less focus on the depth or quality of non-English data. The overall effectiveness in diverse linguistic contexts suggests good generalization capabilities, but the lack of specific details about individual language data might limit the assessment of model performance on each language. The reported decline in performance on English NLU tasks compared to a GPT-3 model trained predominantly on English also points to possible trade-offs involved in broad multilingual training.","English, Chinese (zh), Spanish (es), Hindi (hi), French (fr), German (de), Russian (ru), Arabic (ar), Vietnamese (vi), Thai (th), Urdu (ur), Swahili (sw), Finnish (fi), Bulgarian (bg), Catalan (ca), Portuguese (pt), Japanese (ja), Italian (it), Korean (ko), Estonian (et), Telugu (te), Basque (eu), Indonesian (id), Malay (my), Haitian Creole (ht), Quechua (qu), Bengali (bn), Tamil (ta)",availability of high-quality translations,"GPT-3 (6.7B, Curie, Babbage, Ada), XGLM (7.5B, 2.9B, 1.7B, 564M), mBERT, XLM-R, mT5, mBART"
Language Models are Multilingual Chain-of-Thought Reasoners,"Shi et al., 2022","Paper discusses the languages involved in the pre-training of the models, addresses how language models, specifically GPT-3 and PaLM, have been exposed to these languages during pre-training albeit in varying extents as evidenced by their representation in the pre-training datasets.","Pre-training's influence on multilingual reasoning is key. The paper notes that model performance on MGSM tasks improves with the scale and diversity of pre-training languages. Models like PaLM, pre-trained with a broad language set, show strong multilingual reasoning, even for underrepresented languages.",4,"The study thoroughly discusses the relationship between the models’ training data and their performance on multilingual tasks.
It highlights that despite the lower frequency of underrepresented languages in the training data, the models can perform remarkably well due to the effective generalization capabilities that arise from large-scale and diverse pre-training. However, it also points out that the actual depth of language-specific training data is unclear, which could impact the evaluation of model performance per language. The comparison between performance in high-resource and low-resource languages suggests that while the breadth of language coverage in pre-training helps, the depth of training in specific languages may still be critical.","English (EN)
Bengali (BN)
Chinese (ZH)
French (FR)
German (DE)
Japanese (JA)
Russian (RU)
Spanish (ES)
Swahili (SW)
Telugu (TE)
Thai (TH)","typologically diverse languages with different representation levels in standard pretraining datasets. Typologically diverse not defined though. There is also mention of the distribution of language frequency in the training data for models like PaLM, emphasizing the inclusion of underrepresented languages","GPT-3
PaLM
Codex
mT5-XXL
XLM-R Large
RoBERTa Large
MAD-X Base"
In-context Examples Selection for Machine Translation,"Agrawal et al., 2022","The paper discusses the languages seen during the pre-training phase, specifically referring to the XGLM model. The specific languages are not detailed in the segments provided, but the emphasis is on the model’s capability to handle multilingual input, suggesting a broad training on diverse languages.",The impact isn't discussed in terms of ICL.,1,"The paper provides a methodological analysis of how in-context examples selected from the training dataset affect the output quality of translations.
While it extensively covers how in-context learning manipulates the model’s existing knowledge from pre-training, it lacks a deep dive into the specifics of the training data itself, such as the diversity or representativeness of languages.","English
Italian
Portuguese
Polish
Spanish
French
Chinese
German
Vietnamese
Swahili
Thai
Bengali
Russian
Tamil
Telugu
Estonian
Indonesian
Turkish
Arabic
Hindi
Japanese","availability in datasets + typologically diverse languages
mention of the distribution of language frequency in the training data for models like XGLM and the impact of these distributions on model performance","GPT-3
PaLM-540B
XGLM mT5
XLM-R Large
RoBERTa Large
MAD-X Base
Codex (code-davinci-002)
mBERT
mBART"
Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment,"Tanwar et al., 2023","Addresses that for the used modells, the languages seen during pre-training are crucial for the models’ effectiveness in cross-lingual tasks. The paper notes that while LLMs are typically trained on diverse linguistic data, the specific languages and scripts that were part of the pre-training are not detailed explicitly, which suggests an assumption of broad but unspecified multilingual coverage.",Pre-training's impact is analyzed through cross-lingual in-context learning tasks. Models pre-trained on many languages perform better in cross-lingual settings. The diversity and extent of language coverage in pre-training significantly enhance the models’ generalization ability in ICL tasks.,4,"The study  assesses the relationship between the training data of LLMs and their performance in experiments designed to test cross-lingual ICL.

It proposes that while pre-trained models have a baseline capability due to their extensive training datasets, specific enhancements like X-InSTA can markedly improve performance by better leveraging the models’ pre-trained knowledge.

The study also implies that the full composition and quality of non-English data in the training sets are not fully disclosed or analyzed, which could limit understanding of the models’ true cross-lingual capabilities","German (de)
English (en)
French (fr)
Japanese (ja)
Spanish (es)
Mandarin (zh)",availability in datasets,"XGLM 7.5B
XGLM 1.7B
Bloom 7.1B
mT5"
Boosting Cross-lingual Transferability in Multilingual Models via In-Context Learning,"Kim et al., 2023","Paper explicitly discusses the languages involved in the training of models like XGLM and BLOOM, which were used in their experiments. XGLM was pre-trained with the CC100 corpus that included 30 different languages, and BLOOM was trained on the ROOTS corpus, which consists of 46 languages.",Shows how pre-training on diverse language corpora boosts multilingual models' cross-lingual transfer in in-context learning.,3,"The paper provides a detailed discussion on the effectiveness of using multilingual models for cross-lingual tasks based on their pre-training.
It shows that while these models are capable of remarkable performance in seen languages, they also exhibit strong capabilities in unseen languages when the right prompting strategies (like In-CLT) are employed.


However, the paper also notes a performance discrepancy when transferring knowledge from English to a completely unseen language during pre-training, which points to limitations in the model’s ability to generalize beyond its training data. This suggests that while architectural and training scale factors are leveraged effectively, the specific linguistic diversity of the training data still plays a critical role in determining the model’s performance across different languages.","Arabic (ar)
Chinese (zh)
English (en)
German (de)
Hindi (hi)
Romanian (ro)
Russian (ru)
Spanish (es)
Thai (th)
Turkish (tr)
Vietnamese (vi)","availability in datasets
There is mention of the distribution of language frequency in the training data for models like BLOOM and XGLM, and how these distributions impact model performance and cross-lingual transferability","XGLM
BLOOM"
Multilingual Large Language Models Are Not (Yet) Code-Switchers,"Zhang et al., 2023","Highlights the inadequacy of these models in handling CSW scenarios despite being trained on multilingual datasets. Specifically, it points out that current multilingualism in LLMs does not equate to proficiency with code-switching, suggesting that while multiple languages are part of the training data, the specific nuances and interplay between these languages in CSW contexts are not effectively captured.","While these models perform well in general multilingual tasks, they struggle in CSW scenarios compared to smaller fine-tuned models. This suggests that despite extensive multilingual pre-training, the models aren't well-prepared for CSW's linguistic complexities, revealing a gap in current training methods for in-context adaptability.
",3,"The study effectively critiques the relationship between model training data and the results of CSW experiments. It notes that despite the multilingual training, the models’ ability to handle CSW is limited.
The paper calls for more targeted research to bridge the discrepancy between general multilingual capabilities and specific CSW proficiency. This critique points to the need for training strategies that explicitly incorporate CSW characteristics into the learning process, suggesting that simply having a multilingual dataset is insufficient for mastering CSW.","Spanish
Malayalam
Tamil
Hinglish
English
Hindi
Arabic",availability in datasets,"XLM-R mBERT mDeBERTa mBART-50
M2M100
mT0
BLOOMZ
XGLM
ChatGPT"
MEGA: Multilingual Evaluation of Generative AI,"Ahuja et al., 2023","The paper examines the languages encountered during GPT-4 and BLOOM's pre-training. It highlights the inclusion of a substantial amount of non-English tokens in the pre-training data. For instance, GPT-3's data had tokens from 119 languages, though mostly in English (93%).

Details on the datasets used for evaluating these models emphasize diverse language representation, notably in BLOOM and PaLM, which feature a larger share of non-English data.






","Examines how pre-training affects models across various languages and tasks. Highlights the capability of these models in multilingual contexts and their limitations, especially with low-resource languages using non-Latin scripts. Notes that while GPT-4 shows some improvements, there remains a significant performance gap compared to fine-tuned models, particularly in less represented languages.",4,"The paper  assesses the effectiveness of multilingual models by comparing their performance against state-of-the-art (SOTA) models fine-tuned on specific tasks. It notes the disparity in performance between English and non-English languages, particularly under-resourced ones, and suggests that while the broad multilingual pre-training helps, it is not entirely sufficient for optimal performance across all languages. The study calls for more targeted improvements and better representation of diverse languages in training data to enhance model performance universally.","German (De), English (En), Russian (Ru), French (Fr), Ukrainian (Uk), Italian (It), Spanish (Es), Romanian (Ro), Chinese (Zh), Japanese (Ja), Czech (Cs), Bengali, Hindi, Kannada, Tamil, Telugu, Urdu, Gujarati, Malayalam, Oriya, Punjabi, Assamese, Swahili, Haitian Creole, Quechua, Vietnamese, Greek, Arabic, Estonian, Turkish, Korean, Marathi, Burmese, Thai, Basque, Bulgarian, Indonesian.","typologically diverse, The multilingual capabilities of models are traced back to their pre-training data","ChatGPT (gpt-3.5-turbo-0613), Qwen-7B (Qwen-7B-Chat), Qwen-14B (Qwen-14B-Chat), Qwen-72B (Qwen-72B-Chat), ALMA-13B, Yi-34B (Yi-34B-Chat), mT0-13B (mt0-xxl), Bloomz-176B, GPT-4, GPT-3.5 models (text-davinci-003 and gpt-3.5-turbo), TULRv6, MuRIL, XLM-R Large, mBERT, mT5-Base, multilingual BERT."
Multilingual Few-Shot Learning via Language Model Retrieval,"Winata et al., 2023","Paper discusses the pre-training languages of the models -- multilingual model XGLM and the primarily English GPT-J NEO. It mentions that XGLM was pre-trained on data from a variety of languages, reflecting its capability to handle multilingual tasks. This includes the application in both monolingual and cross-lingual in-context learning settings, where the model retrieves and utilizes prompts from languages that may or may not be the same as the query language.","Explores the impact of pre-training on in-context learning of language models. Shows that XGLM, with multilingual pre-training, excels in cross-lingual tasks by using semantically similar prompts across languages. Highlights that the breadth and diversity of the pre-training corpus significantly influence model performance in multilingual settings.

",5,"The study  examines the effectiveness of the training data in relation to the experimental results. It highlights how the choice of semantically similar prompts, derived from the multilingual training data, enhances the models’ performance in NLU tasks across different languages.
However, the paper also points out the variability in performance based on the semantic similarity of the prompts, suggesting that while the models are capable of using the training data effectively, the quality and selection of prompts are crucial for optimal performance. This underscores the need for carefully curated training sets that are representative of real-world language use in diverse linguistic contexts.","English, French, German, Spanish.","The rationale is based on demonstrating the model’s ability to leverage semantically similar examples from different languages, rather than on specific characteristics of the languages themselves","GPT-J NEO (1.3B), XGLM (1.7B), XLM-RBASE."
Large Pre-trained Language Models with Multilingual Prompt for Japanese Natural Language Tasks,"Song et al., 2023","Discusses on the dominance of English in the datasets used for pre-training large models like GPT-3. It acknowledges the challenge this poses for languages other than English, such as Japanese, in achieving comparable performance levels. The context given is in relation to the performance disparity between English and non-English tasks, emphasizing the underrepresentation of languages like Japanese in the training data.","Impact of pre-training on in-context learning is  examined, especially how it affects languages that are not well-represented in the training data.",4,"The study assesses the effectiveness of the multilingual training approach by comparing it with standard fine-tuning methods on various Japanese NLU tasks.
It finds that while the proposed method improves performance on certain tasks, it highlights an ongoing challenge with performance consistency across different types of tasks, particularly those requiring deep semantic understanding or complex reasoning.
This inconsistency points to the limitations in training data diversity and the need for more targeted strategies to improve the model’s performance across a broader spectrum of tasks.","Japanese, English.",,"GPT-3 Codex (175B), TexTra, Tohoku BERT, NICT BERT base, Waseda RoBERTa, XLM RoBERTa, CRF+BERT, MeCab, KyTea, Jumanpp."
How Good are Commercial Large Language Models on African Languages?,"Ojo & Ogueji, 2023","Paper examines languages in the pre-training of commercial large language models like GPT-3. Notes that performance on African languages is poorly understood, suggesting these languages are underrepresented or absent in pre-training.

This gap in language diversity indicates African languages are not sufficiently covered during the models' pre-training phase.","Examines examines how pre-training affects underrepresented languages in training data. Models perform poorly on tasks like machine translation and text classification for African languages, indicating that pre-training didn't equip them to handle these languages effectively. Points out the limitation in models' ability to generalize across less-represented languages.",3,"The paper  assesses the effectiveness of the training data by showing that the language models, which are likely trained predominantly on high-resource languages, do not perform well when tested on low-resource, underrepresented African languages. This discrepancy highlights the issue that while the models are capable of impressive performances in languages with ample training data, their ability to transfer learning to significantly different languages is limited. The results call for a more inclusive approach in training data selection to improve performance across a broader spectrum of languages.","Hausa, Nigerian Pidgin, Malagasay, Somali, isiZulu, Yoruba, Swahili, Lugala, English.",The paper explains that the languages chosen for evaluation cover different language families and geographical areas. This selection is to provide a diverse set of African languages for analysis,"ChatGPT, Cohere, GPT-3, BERT, RoBERTa, T5, PaLM."
"BUFFET: Benchmarking Large Language Models
for Few-shot Cross-lingual Transfer","Asai et al., 2023","Paper does not detail specific languages seen during pre-training for each model,","The impact of pre-training is a significant focus. The paper assesses how well various large language models, pre-trained on diverse data, perform on cross-lingual few-shot tasks using both in-context learning and fine-tuning methods.
It highlights the challenges and limitations encountered in few-shot cross-lingual transfer, especially for languages that are distant from the source language or are low-resource. The findings suggest that despite extensive pre-training, models often struggle with tasks in less familiar languages, indicating a gap between pre-training diversity and effective cross-lingual applicability.",5,"The paper provides a critical view of the training data’s effectiveness by discussing the performance variations of models across different tasks and languages.
It notes that while models like mT5 and BLOOM show promising results in some languages, their performance is inconsistent across the full spectrum of languages tested. This inconsistency points to potential shortcomings in the training data’s language coverage or the models’ ability to leverage this data effectively for in-context learning and cross-lingual transfer.","English, Arabic, Swahili, Japanese, Telugu, Finnish, Korean, Russian, Bengali, Vietnamese, Chinese, Hausa, Yoruba, Somali, isiZulu, Luganda, Malagasy, Nigerian Pidgin.",,"ChatGPT, BLOOM-7B, BLOOMZ-7B, mT5-base, mT5-xxl, mT0-xxl, gpt-3.5-turbo, BLOOMZ, BLOOM, XGLM, XLM-R, PaLM."
Prompting Large Language Model for Machine Translation: A Case Study,"Zhang et al., 2023","The paper discusses the languages seen during the pre-training of the GLM-130B model used for machine translation prompting. It highlights that the model was trained on Chinese and English monolingual corpora, providing a detailed context of the model’s linguistic exposure. This background is crucial for understanding the model’s capabilities and limitations in handling translations between these languages and potentially others ￼.","Impact of pre-training is explored through the lens of machine translation performance. The study illustrates how the pre-trained model’s effectiveness varies across different language pairs, particularly noting that translations involving German and Chinese show different levels of effectiveness. This discrepancy underlines the influence of the pre-training languages (English and Chinese) on the model’s ability to perform in-context learning tasks across different language contexts ￼.",4,Paper provides a critical evaluation of how well the pre-training on English and Chinese supports the model’s performance across various translation tasks. It discusses the limitations observed when the model is prompted for translations involving languages that were not part of its primary training data. This assessment is significant for understanding the boundaries of the model’s capabilities and the need for potentially enhancing the training datasets to include more diverse linguistic data ￼.,"English, German, and Chinese","Past Performacne
The paper mentions that the cross-lingual ability of the GLM-130B centers around English, despite the model being pretrained on both English and Chinese. This suggests that the amount and type of pretraining data influence the effectiveness of different language templates","mT5 (base), mT5 (xxl), BLOOM, BLOOMZ (7B), mT0 (13B), ChatGPT, GPT-3.5-turbo, GPT-4 (32k), TULRv6, XLM-R, multilingual BERT, MuRIL, GLM-130B."
"M3Exam: A Multilingual, Multimodal, Multilevel
Benchmark for Examining Large Language Models",Zhang et al. 2023,Paper analyzes large language models' performance on the M3Exam benchmark. Focuses on evaluating models across diverse languages and educational levels. Suggests extensive linguistic diversity in pre-training. Lacks specific details on languages or exact composition of training datasets in the provided sections.,"Inferred through the models’ performance on the M3Exam, which indicates significant challenges when processing multilingual text, especially in low-resource and non-Latin script languages. This suggests that while the models have been exposed to diverse languages during pre-training, there may be gaps in their ability to effectively apply this training to complex multilingual and multimodal in-context tasks.",3,"Paper provides a critical look at the relationship between the models’ training data and their performance in the exams. It highlights the deficiencies of current models in dealing with complex, real-world multilingual and multimodal problems as presented in the M3Exam.

The discussion implies that while the pre-training includes diverse languages, the depth of understanding and the ability to handle real-world context in these languages may be lacking, pointing to a need for more targeted training approaches or enhanced model architectures to better handle such diversity.","English, Chinese, Italian, Portuguese, Vietnamese, Thai, Swahili, Afrikaans, Javanese.","selection aims to cover a wide range of language families, resources, written scripts, and major spoken countries to comprehensively evaluate the multilingual capabilities","ChatGPT (gpt-3.5-turbo), GPT-4, Claude (Claude-instant), BLOOM (176B), Vicuna (13B), BLIP-2, InstructBLIP, Fromage, OpenFlamingo."
"Breaking Language Barriers with a LEAP:
Learning Strategies for Polyglot LLMs","Nambi et al., 2023","The paper highlights the languages seen during pre-training, focusing on the challenges faced by LLMs with non-Latin scripts and low-resource languages. It emphasizes that while LLMs like GPT-3 and GPT-4 have broad training, there's an underrepresentation of non-English languages. This underrepresentation affects their performance in diverse linguistic tasks, setting the stage for experiments aimed at enhancing multilingual capabilities.","Discusses impact of pre-training on in-context learning abilities of LLMs through the introduction of novel strategies aimed at enhancing polyglot capabilities. These strategies are tested across various language contexts to evaluate their effectiveness in unlocking the latent potential of LLMs. By optimizing prompts and integrating hybrid techniques that combine GPT generation with multilingual embeddings, the study demonstrates significant performance improvements in multilingual tasks.",4,"Study provides a critical assessment of how effectively the pre-trained data supports the experimental results. Despite the broad training base of models like GPT-3 and GPT-4, the paper identifies specific gaps in performance when dealing with tasks that require deep multilingual understanding, particularly in low-resource languages. The results show that while these models perform well in high-resource languages, their effectiveness varies significantly with less represented languages, indicating limitations in the training data’s diversity and depth.","English, Chinese, Italian, Portuguese, Vietnamese, Thai, Swahili, Afrikaans, Javanese, Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, Telugu.",mentions that the performance of multilingual models is influenced by the availability and quality of training data for each language. It highlights the challenges faced by LLMs in handling medium and low-resource languages,"GPT-3 (text-davinci-003), GPT-3.5 Turbo, GPT-4, Muril, XLM-R, IndicBERT, TuLR, TULRv6, ChatGPT."
"DEMOCRATIZING LLMS FOR LOW-RESOURCE LAN-
GUAGES BY LEVERAGING THEIR ENGLISH DOMINANT
ABILITIES WITH LINGUISTICALLY-DIVERSE PROMPTS","Nguyen et al., 2023","The paper explicitly mentions that while LLMs have been pre-trained with multilingual corpora, the majority of the data is in English, leading to an imbalance in performance across different languages. The models show strong generative capabilities in high-resource languages but struggle with low-resource languages due to this pre-training data imbalance. Specifically, it mentions the use of the ROOTS corpus, which includes languages with varying levels of representation, highlighting Indic and African languages like Hindi and Tumbuka.","The impact of pre-training is a central theme in the paper. It demonstrates how the imbalance in pre-training data affects the models' abilities in low-resource languages. LDP propting results show that LLMs can achieve competitive performance with supervised few-shot learning even in zero-shot setups, indicating the significant impact of pre-training data diversity and quality on in-context learning abilities.",5,"The paper provides a detailed discussion of the training data's effectiveness in supporting the models' performance in experiments. It highlights that while the models are pre-trained on a large and diverse corpus, the performance in low-resource languages is still suboptimal due to the limited representation of these languages in the training data. The experiments with LDP demonstrate improved results, suggesting that the inclusion of linguistically diverse prompts can mitigate some of the limitations posed by the pre-training data imbalance. However, the study also acknowledges that the models sometimes generate incorrect languages or struggle with non-Latin scripts, pointing to ongoing challenges in achieving truly robust multilingual capabilities.","Arabic, Chinese, Vietnamese, French, English, German, Russian, Ukrainian, Italian, Spanish, French, Nepali, Punjabi, Swahili, Tamil, Telugu, Indonesian, Thai, Javanese, Swahili, Aymara, Korean, Finnish, Russian, Bengali, Hausa",,"BLOOM-175B, BLOOM-7B1, GPT-3 (text-davinci-003), GPT-3.5-turbo, ChatGPT, Claude, Vicuna, mT5, LLaMA-30B, XLM-RoBERTa, Qwen-14B, TULRv6, ChatGPT-4, InstructGPT (text-davinci-003)"
The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis,"Zhang et al., 2024","The paper discusses the languages involved in the training of LLMs used for multilingual contexts. It notes that while LLMs are trained on diverse linguistic data, the specific languages and scripts in the pre-training are not detailed explicitly, suggesting an assumption of broad but unspecified multilingual coverage​​.","The impact of pre-training is analyzed through the performance of these models in cross-lingual in-context learning tasks. The paper highlights that models pre-trained on a wide array of languages tend to perform better in cross-lingual settings, especially when aligned strategies like Cross-lingual In-context Source-Target Alignment (X-InSTA) are employed. This suggests that the diversity and extent of language coverage in pre-training significantly influence the models' ability to generalize across languages in ICL tasks​​.",4,"The study assesses the relationship between the training data of LLMs and their performance in experiments designed to test cross-lingual ICL. It proposes that while pre-trained models have a baseline capability due to their extensive training datasets, specific enhancements like X-InSTA can markedly improve performance by better leveraging the models' pre-trained knowledge. The paper argues that traditional random prompt selection methods do not sufficiently exploit the cross-lingual potential of LLMs, and that more targeted prompting strategies can bridge this gap. However, the study also implies that the full composition and quality of non-English data in the training sets are not fully disclosed or analyzed, which could limit understanding of the models' true cross-lingual capabilities​​.","English, Arabic, Chinese, French, German, Hindi, Indonesian, Japanese, Korean, Russian, Spanish, Swahili, Thai, Turkish, Vietnamese, Xhosa, Yoruba, Zulu.",no rationale for the specific choice of each language,"XGLM (7.5B), Llama 2 (13B), Llama 2-Chat (13B), GPT-3.5, GPT-4."
Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis,"Zhu et al., 2024","The paper discusses the languages the models have seen during pre-training, focusing on their capabilities and limitations with multilingual data. It notes that while the models are pre-trained on various languages, they show unbalanced capabilities, particularly struggling with low-resource languages. This indicates varying degrees of exposure during their training phase.","Discusses the impact of pre-training extensively, especially in relation to in-context learning ability. It mentions that while the models have shown impressive capabilities in multilingual settings due to their pre-training, there are notable performance discrepancies based on the language resources available during training. This highlights that the depth and breadth of pre-training data significantly influence the models' ability to handle in-context learning tasks across different languages.",5,"The study appears to  discuss the model training data with respect to the results of the experiment. It points out that despite high performance in certain language pairs, the models struggle with low-resource languages, indicating limitations in the training data's linguistic diversity. The paper also notes the need for models to better generalize across diverse languages, suggesting a gap between the current training data composition and the models' experimental performance in real-world multilingual tasks.","English, German, Russian, French, Chinese Simplified, Spanish, Japanese, Italian, Vietnamese, Turkish, Indonesian, Swahili, Arabic, Korean, Greek, Thai, Bulgarian, Hindi, Estonian, Bengali, Tamil, Galician, Urdu, Telugu, Javanese, Haitian Creole, Southern Quechua, Catalan, Asturian, Dutch, Danish, Swedish, Icelandic, Luxembourgish, Norwegian.",,"XGLM-7.5B, OPT-175B, Falcon-7B, LLaMA2-7B, LLaMA2-7B-chat, ChatGPT, GPT-4, M2M-100-12B, NLLB-1.3B, Google Translator, BLOOMZ-7.1B."
Large Language Models are Parallel Multilingual Learners,"Mu et al., 2024","The paper explicitly discusses the languages involved in the training and experiments. It states that the models were primarily trained on English data but were tested for multilingual capability across English, French, German, and Spanish. It focuses on how models like GPT and T5 handle these languages in monolingual and cross-lingual settings, suggesting some degree of exposure to these languages during their training phase.","Central theme of the paper./// It examines how well the language models, which were pre-trained on predominantly English data, perform on multilingual few-shot learning tasks without any additional training. The paper indicates that larger models tend to perform better, suggesting that the volume and diversity of pre-training data play crucial roles in the models’ ability to generalize across languages. This is particularly evident in the cross-lingual tasks where the models were tested on language pairs that involve translating or understanding across languages they were not specifically fine-tuned on.",5,"Paper shows a reasonably detailed discussion about the effectiveness of using pre-trained models on multilingual NLU tasks. However, while it highlights the capabilities of these models to perform with few-shot examples in languages that are not part of the extensive training regime, it doesn’t deeply analyze or disclose the specific makeup or comprehensiveness of the non-English data in the training sets. The effectiveness in non-English languages, as presented, seems to hinge more on the models’ architectural capabilities and size rather than a robust, linguistically diverse training dataset. This might pose limitations in evaluating the true linguistic adaptability of the models beyond surface-level patterns or similarities to English.","German (De)
English (En)
Russian (Ru)
French (Fr)
Ukrainian (Uk)
Italian (It)
Spanish (Es)
Romanian (Ro)
Chinese (Zh)
Japanese (Ja)
Czech (Cs)",choice of languages in the PIM approach is influenced by the model’s pre-training,"ChatGPT (gpt-3.5-turbo-0613), Qwen-7B (Qwen-7B-Chat), Qwen-14B (Qwen-14B-Chat), Qwen-72B (Qwen-72B-Chat), ALMA-13B, Yi-34B (Yi-34B-Chat), mT0-13B (mt0-xxl), Bloomz-176B, and GPT-4."
"Hire a Linguist!: Learning Endangered Languages
with In-Context Linguistic Descriptions","Zhang et al., 2024","The paper discusses that LLMs typically have significant exposure to high-resource languages such as English and Spanish during pre-training. It emphasizes the poor performance of these models on endangered languages that are rarely seen during pre-training, highlighting the limitations of LLMs with low-resource languages.","Details how LLMs, pre-trained primarily on high-resource languages, struggle with tasks in endangered languages due to the lack of exposure to these languages during their training. This scenario is used to argue for the necessity of the proposed approach, which utilizes linguistic descriptions (like grammar books and dictionaries of endangered languages) to enhance the models' capabilities without additional training. This method aims to compensate for the insufficiencies of pre-training by directly integrating linguistic knowledge into the LLM's processes​​.",4,"The study assesses the disparity between the training data of LLMs and their performance on endangered languages. It highlights that while LLMs are trained on extensive data from well-represented languages, their ability to generalize to languages absent from their training data is notably poor. The proposed method, LINGOLLM, leverages external linguistic resources to bridge this gap, demonstrating significant improvements in translation and other NLP tasks across multiple endangered languages. This approach showcases an innovative use of linguistic knowledge to enhance the functionality of LLMs in domains where they traditionally perform poorly due to the limitations of their training data​​.","Manchu, Gitksan, Uspanteko, Natugu, Tsez, Wolof, Arapaho, Bribri, English, Spanish, Chinese.",,"GPT-4, Mixtral."
"Not All Languages Are Created Equal in LLMs:
Improving Multilingual Capability by Cross-Lingual-Thought Prompting",Huang,"The paper notes that current LLMs primarily leverage extensive English language corpora but also include data from various other languages. This inclusion is generalized rather than specific, and the paper does not detail which additional languages were used during pre-training. It mentions the need to improve performance, especially for low-resource languages​​.","Paper implicitly suggests that the foundational pre-training in multiple languages facilitates the use of XLT to improve task performance across languages by leveraging cross-lingual and logical reasoning skills. However, it does not directly analyze the pre-training impact in a comparative manner concerning in-context learning abilities​​.",2,"The study critiques the standard approach of model training by highlighting the limitations in language coverage and the dominance of English in the training data. It points out that despite the inclusion of other languages, there is an imbalance in capability across languages, which the XLT method aims to mitigate. The effectiveness of this approach is demonstrated through improved performance across multiple benchmarks, suggesting a successful application of the training data to enhance multilingual task performance. However, the paper does not provide a deep dive into the specific makeup or comprehensive details of the non-English data in the training sets, which aligns with a common gap found in similar research​​.","English, German, Russian, French, Chinese Simplified, Spanish, Japanese, Italian, Vietnamese, Turkish, Indonesian, Swahili, Arabic, Korean, Greek, Thai, Bulgarian, Hindi, Estonian, Bengali, Tamil, Galician, Urdu, Telugu, Javanese, Haitian Creole, Southern Quechua.",,"text-davinci-003, gpt-3.5-turbo, LLaMA-2-70b-chat-hf."